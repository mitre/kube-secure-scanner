stages:
  - deploy
  - scan
  - verify
  - cleanup

variables:
  KUBERNETES_NAMESPACE: "dynamic-scan-$CI_PIPELINE_ID"
  TARGET_IMAGE: "busybox:latest"
  SCAN_LABEL_KEY: "scan-target"
  SCAN_LABEL_VALUE: "true"
  CINC_PROFILE: "dev-sec/linux-baseline"
  THRESHOLD_VALUE: "70"  # Minimum passing score (0-100)
  DURATION_MINUTES: "15" # Token duration in minutes

# Allow overriding variables through pipeline triggers or UI
.dynamic_variables: &dynamic_variables
  TARGET_IMAGE: ${TARGET_IMAGE}
  SCAN_LABEL_KEY: ${SCAN_LABEL_KEY}
  SCAN_LABEL_VALUE: ${SCAN_LABEL_VALUE}
  CINC_PROFILE: ${CINC_PROFILE}
  THRESHOLD_VALUE: ${THRESHOLD_VALUE}
  ADDITIONAL_PROFILE_ANNOTATION: "${ADDITIONAL_PROFILE_ANNOTATION}"  # Optional annotation for specifying additional profiles

setup_test_environment:
  stage: deploy
  script:
    - echo "$KUBE_CONFIG" | base64 -d > kubeconfig.yaml
    - export KUBECONFIG=kubeconfig.yaml
    
    # Create test namespace
    - kubectl create namespace ${KUBERNETES_NAMESPACE}
    
    # Create multiple test pods with different images and labels
    - |
      # Create 3 pods, but only mark the first one for scanning
      for i in {1..3}; do
        cat <<EOF | kubectl apply -f -
        apiVersion: v1
        kind: Pod
        metadata:
          name: pod-${i}
          namespace: ${KUBERNETES_NAMESPACE}
          labels:
            app: test-pod-${i}
            ${SCAN_LABEL_KEY}: "$([ $i -eq 1 ] && echo "${SCAN_LABEL_VALUE}" || echo "false")"
          annotations:
            scan-profile: "${CINC_PROFILE}"
            $([ -n "${ADDITIONAL_PROFILE_ANNOTATION}" ] && echo "${ADDITIONAL_PROFILE_ANNOTATION}" || echo "")
        spec:
          containers:
          - name: container
            image: ${TARGET_IMAGE}
            command: ["sleep", "infinity"]
        EOF
      done
    
    # Wait for pods to be ready
    - kubectl wait --for=condition=ready pod -l app=test-pod-1 -n ${KUBERNETES_NAMESPACE} --timeout=120s
    
    # Get the name of the pod with our scan label
    - |
      TARGET_POD=$(kubectl get pods -n ${KUBERNETES_NAMESPACE} -l ${SCAN_LABEL_KEY}=${SCAN_LABEL_VALUE} -o jsonpath='{.items[0].metadata.name}')
      if [ -z "$TARGET_POD" ]; then
        echo "Error: No pod found with label ${SCAN_LABEL_KEY}=${SCAN_LABEL_VALUE}"
        exit 1
      fi
      echo "TARGET_POD=${TARGET_POD}" >> deploy.env
    
    # Save scan profile from annotations if available
    - |
      SCAN_PROFILE=$(kubectl get pod ${TARGET_POD} -n ${KUBERNETES_NAMESPACE} -o jsonpath='{.metadata.annotations.scan-profile}')
      if [ -n "$SCAN_PROFILE" ]; then
        echo "Found scan profile annotation: ${SCAN_PROFILE}"
        echo "SCAN_PROFILE=${SCAN_PROFILE}" >> deploy.env
      else
        echo "Using default profile: ${CINC_PROFILE}"
        echo "SCAN_PROFILE=${CINC_PROFILE}" >> deploy.env
      fi
    
    # Show all pods in the namespace
    - kubectl get pods -n ${KUBERNETES_NAMESPACE} --show-labels
  artifacts:
    reports:
      dotenv: deploy.env

create_dynamic_rbac:
  stage: scan
  needs: [setup_test_environment]
  script:
    - echo "$KUBE_CONFIG" | base64 -d > kubeconfig.yaml
    - export KUBECONFIG=kubeconfig.yaml
    
    # Create service account
    - |
      cat <<EOF | kubectl apply -f -
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: scanner-sa
        namespace: ${KUBERNETES_NAMESPACE}
      EOF
    
    # Create role with label-based access
    - |
      cat <<EOF | kubectl apply -f -
      apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: scanner-role
        namespace: ${KUBERNETES_NAMESPACE}
      rules:
      - apiGroups: [""]
        resources: ["pods"]
        verbs: ["get", "list"]
      - apiGroups: [""]
        resources: ["pods/exec"]
        verbs: ["create"]
      - apiGroups: [""]
        resources: ["pods/log"]
        verbs: ["get"]
      EOF
    
    # Create role binding
    - |
      cat <<EOF | kubectl apply -f -
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: scanner-binding
        namespace: ${KUBERNETES_NAMESPACE}
      subjects:
      - kind: ServiceAccount
        name: scanner-sa
        namespace: ${KUBERNETES_NAMESPACE}
      roleRef:
        kind: Role
        name: scanner-role
        apiGroup: rbac.authorization.k8s.io
      EOF
    
    # Generate token
    - |
      TOKEN=$(kubectl create token scanner-sa -n ${KUBERNETES_NAMESPACE} --duration=${DURATION_MINUTES}m)
      SERVER=$(kubectl config view --minify --output=jsonpath='{.clusters[0].cluster.server}')
      CA_DATA=$(kubectl config view --raw --minify --flatten -o jsonpath='{.clusters[].cluster.certificate-authority-data}')
      
      # Save token and cluster info for later stages
      echo "SCANNER_TOKEN=${TOKEN}" >> scanner.env
      echo "CLUSTER_SERVER=${SERVER}" >> scanner.env
      echo "CLUSTER_CA_DATA=${CA_DATA}" >> scanner.env
  artifacts:
    reports:
      dotenv: scanner.env

run_security_scan:
  stage: scan
  needs: [setup_test_environment, create_dynamic_rbac]
  script:
    # Create kubeconfig with restricted token
    - |
      cat > scan-kubeconfig.yaml << EOF
      apiVersion: v1
      kind: Config
      preferences: {}
      clusters:
      - cluster:
          server: ${CLUSTER_SERVER}
          certificate-authority-data: ${CLUSTER_CA_DATA}
        name: scanner-cluster
      contexts:
      - context:
          cluster: scanner-cluster
          namespace: ${KUBERNETES_NAMESPACE}
          user: scanner-user
        name: scanner-context
      current-context: scanner-context
      users:
      - name: scanner-user
        user:
          token: ${SCANNER_TOKEN}
      EOF
      chmod 600 scan-kubeconfig.yaml
    
    # Install CINC Auditor
    - curl -L https://omnitruck.cinc.sh/install.sh | sudo bash -s -- -P cinc-auditor
    
    # Install train-k8s-container plugin
    - cinc-auditor plugin install train-k8s-container
    
    # Install SAF CLI
    - npm install -g @mitre/saf
    
    # Verify the tools
    - cinc-auditor --version
    - saf --version
    
    # Find the target pod by label using the restricted token
    - |
      echo "Looking for pods with label: ${SCAN_LABEL_KEY}=${SCAN_LABEL_VALUE}"
      SCANNED_POD=$(KUBECONFIG=scan-kubeconfig.yaml kubectl get pods -n ${KUBERNETES_NAMESPACE} -l ${SCAN_LABEL_KEY}=${SCAN_LABEL_VALUE} -o jsonpath='{.items[0].metadata.name}')
      if [ -z "$SCANNED_POD" ]; then
        echo "Error: No pod found with label ${SCAN_LABEL_KEY}=${SCAN_LABEL_VALUE} using restricted access"
        exit 1
      fi
      echo "Found target pod: ${SCANNED_POD}"
      
      # Verify it matches what we expected
      if [ "$SCANNED_POD" != "$TARGET_POD" ]; then
        echo "Warning: Scanned pod ($SCANNED_POD) doesn't match expected target pod ($TARGET_POD)"
      fi
    
    # Get container name
    - CONTAINER_NAME=$(kubectl get pod ${TARGET_POD} -n ${KUBERNETES_NAMESPACE} -o jsonpath='{.spec.containers[0].name}')
    
    # Run CINC Auditor scan
    - |
      echo "Running CINC Auditor scan on ${KUBERNETES_NAMESPACE}/${TARGET_POD}/${CONTAINER_NAME}"
      KUBECONFIG=scan-kubeconfig.yaml cinc-auditor exec ${SCAN_PROFILE} \
        -t k8s-container://${KUBERNETES_NAMESPACE}/${TARGET_POD}/${CONTAINER_NAME} \
        --reporter cli json:scan-results.json
      
      # Save scan exit code
      SCAN_EXIT_CODE=$?
      echo "SCAN_EXIT_CODE=${SCAN_EXIT_CODE}" >> scan.env
      echo "Scan completed with exit code: ${SCAN_EXIT_CODE}"
    
    # Process results with SAF-CLI
    - |
      echo "Generating scan summary with SAF-CLI:"
      saf summary --input scan-results.json --output-md scan-summary.md
      
      # Display the summary in the logs
      cat scan-summary.md
      
      # Create a threshold file
      cat > threshold.yml << EOF
      compliance:
        min: ${THRESHOLD_VALUE}
      failed:
        critical:
          max: 0  # No critical failures allowed
      EOF
      
      # Apply threshold check
      echo "Checking against threshold with min compliance of ${THRESHOLD_VALUE}%:"
      saf threshold -i scan-results.json -t threshold.yml
      THRESHOLD_RESULT=$?
      echo "THRESHOLD_RESULT=${THRESHOLD_RESULT}" >> scan.env
      
      if [ $THRESHOLD_RESULT -eq 0 ]; then
        echo "✅ Security scan passed threshold requirements"
      else
        echo "❌ Security scan failed to meet threshold requirements"
        # Uncomment to enforce the threshold as a quality gate
        # exit $THRESHOLD_RESULT
      fi
      
      # Generate comprehensive HTML report
      saf view -i scan-results.json --output scan-report.html
  artifacts:
    paths:
      - scan-results.json
      - scan-summary.md
      - scan-report.html
    reports:
      dotenv: scan.env

verify_rbac_restrictions:
  stage: verify
  needs: [setup_test_environment, create_dynamic_rbac, run_security_scan]
  script:
    - echo "$KUBE_CONFIG" | base64 -d > kubeconfig.yaml
    - export KUBECONFIG=kubeconfig.yaml
    
    # Create a second kubeconfig with restricted token
    - |
      cat > verify-kubeconfig.yaml << EOF
      apiVersion: v1
      kind: Config
      preferences: {}
      clusters:
      - cluster:
          server: ${CLUSTER_SERVER}
          certificate-authority-data: ${CLUSTER_CA_DATA}
        name: scanner-cluster
      contexts:
      - context:
          cluster: scanner-cluster
          namespace: ${KUBERNETES_NAMESPACE}
          user: scanner-user
        name: scanner-context
      current-context: scanner-context
      users:
      - name: scanner-user
        user:
          token: ${SCANNER_TOKEN}
      EOF
      chmod 600 verify-kubeconfig.yaml
    
    # Get a non-target pod name
    - OTHER_POD=$(kubectl get pods -n ${KUBERNETES_NAMESPACE} -l app=test-pod-2 -o jsonpath='{.items[0].metadata.name}')
    
    # Check what we CAN do
    - |
      echo "Verifying what we CAN do with restricted RBAC:"
      echo "Can list pods:"
      KUBECONFIG=verify-kubeconfig.yaml kubectl get pods -n ${KUBERNETES_NAMESPACE} > /dev/null && 
        echo "✅ Can list pods" || 
        echo "❌ Cannot list pods"
      
      echo "Can exec into target pod:"
      KUBECONFIG=verify-kubeconfig.yaml kubectl auth can-i create pods/exec --subresource=exec -n ${KUBERNETES_NAMESPACE} --resource-name=${TARGET_POD} &&
        echo "✅ Can exec into target pod" || 
        echo "❌ Cannot exec into target pod"
    
    # Check what we CANNOT do
    - |
      echo "Verifying what we CANNOT do with restricted RBAC:"
      echo "Cannot create pods:"
      KUBECONFIG=verify-kubeconfig.yaml kubectl auth can-i create pods -n ${KUBERNETES_NAMESPACE} && 
        echo "❌ Security issue: Can create pods" || 
        echo "✅ Cannot create pods (expected)"
      
      echo "Cannot delete pods:"
      KUBECONFIG=verify-kubeconfig.yaml kubectl auth can-i delete pods -n ${KUBERNETES_NAMESPACE} && 
        echo "❌ Security issue: Can delete pods" || 
        echo "✅ Cannot delete pods (expected)"
    
    # Create a security report for MR
    - |
      cat > security-report.md << EOF
      # Container Security Scan Report
      
      ## Scan Results
      
      $(cat scan-summary.md)
      
      ## Threshold Check
      
      $([[ "${THRESHOLD_RESULT}" -eq 0 ]] && echo "✅ **PASSED**" || echo "❌ **FAILED**")
      
      Threshold: ${THRESHOLD_VALUE}%
      
      ## RBAC Security Verification
      
      The scanner service account has properly restricted access:
      - ✅ Can list pods in the namespace
      - ✅ Can exec into target pods for scanning
      - ✅ Cannot create or delete pods
      - ✅ Cannot access cluster-wide resources
      
      ## Scan Details
      
      - Target Pod: \`${TARGET_POD}\`
      - Container: \`${CONTAINER_NAME}\`
      - Image: \`${TARGET_IMAGE}\`
      - Profile: \`${SCAN_PROFILE}\`
      
      For full results, see the scan artifacts.
      EOF
  artifacts:
    paths:
      - security-report.md
      
cleanup:
  stage: cleanup
  needs: [setup_test_environment]
  when: always  # Run even if previous stages failed
  script:
    - echo "$KUBE_CONFIG" | base64 -d > kubeconfig.yaml
    - export KUBECONFIG=kubeconfig.yaml
    - kubectl delete namespace ${KUBERNETES_NAMESPACE} --ignore-not-found